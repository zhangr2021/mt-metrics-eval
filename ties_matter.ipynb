{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-4ZLr6A3_Yy"
      },
      "source": [
        "# Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration\n",
        "This Colab noteook contains the code for reproducing the results from the publication \"Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration\" by Daniel Deutsch, George Foster, and Markus Freitag.\n",
        "\n",
        "If you just want to use tie calibration and pairwise accuracy as proposed in the paper, here is an example of the most direct way of doing so:\n",
        "\n",
        "```python\n",
        "from mt_metrics_eval import tau_optimization\n",
        "from mt_metrics_eval import stats\n",
        "\n",
        "# M is the number of groups, N is the number of observations per group.\n",
        "# For instance, in the case of the group-by-item correlations, M is the number\n",
        "# of items (or segments) and N is the number of systems. If you have no\n",
        "# groups, M=1.\n",
        "M = 10\n",
        "N = 20\n",
        "\n",
        "# Generate fake data for this example. X should be the matrix of metric scores\n",
        "# and Y should be the matrix of human scores\n",
        "X = np.random.rand(M, N)\n",
        "Y = np.random.rand(M, N)\n",
        "\n",
        "# Run tie calibration (called tau_optimization in this notebook). The sample_rate\n",
        "# parameter indicates what proportion of all possible pairs of observations\n",
        "# to use when searching for the optimal epsilon. If 1.0, all will be used. If\n",
        "# you have a very large number of pairs, you may want to lower this. We found\n",
        "# that small values (0.1) actually tend to yield relatively stable results\n",
        "# (see Appendix E in the paper).\n",
        "sample_rate = 1.0\n",
        "result = tau_optimization.tau_optimization(\n",
        "  X, Y, tau_optimization.TauSufficientStats.acc_23,\n",
        ")\n",
        "\n",
        "# The result object has various information, including `best_threshold` (equal\n",
        "# to the best epsilon), `best_tau` (equal to the best `acc_23` score), plus\n",
        "# `thresholds` and `taus` (they contain the various epsilsons and corresponding\n",
        "# accuracy scores that were used in the search).\n",
        "print(result.best_threshold)\n",
        "print(result.best_tau)\n",
        "\n",
        "# If you already have an epsilon and you want to compute an accuracy score\n",
        "# with that epsilon with two vectors, then you can do so with the following:\n",
        "x = np.random.rand(N)  # the metric scores\n",
        "y = np.random.rand(N)  # the human scores\n",
        "epsilon = 0.05\n",
        "accuracy, _ = stats.KendallVariants(\n",
        "  Y, X, variant=\"acc23\", epsilon=epsilon\n",
        ")\n",
        "print(accuracy)\n",
        "```\n",
        "\n",
        "If you use this meta-evaluation methodology, please cite the following paper:\n",
        "```\n",
        "@misc{deutsch2023ties,\n",
        "      title={{Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration}},\n",
        "      author={Daniel Deutsch and George Foster and Markus Freitag},\n",
        "      year={2023},\n",
        "      eprint={2305.14324},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CL}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "696rbZ11E6-l"
      },
      "source": [
        "## Environment Setup\n",
        "Installs the MTME library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mt_metrics_eval import tau_optimization\n",
        "from mt_metrics_eval import stats\n",
        "\n",
        "# M is the number of groups, N is the number of observations per group.\n",
        "# For instance, in the case of the group-by-item correlations, M is the number\n",
        "# of items (or segments) and N is the number of systems. If you have no\n",
        "# groups, M=1.\n",
        "M = 2 # number of unique sources\n",
        "N = 5 # number of systems\n",
        "\n",
        "# Generate fake data for this example. X should be the matrix of metric scores\n",
        "# and Y should be the matrix of human scores\n",
        "X = np.random.rand(M, N)\n",
        "Y = np.random.rand(M, N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n",
            "0.5\n",
            "29.0\n"
          ]
        }
      ],
      "source": [
        "# Run tie calibration (called tau_optimization in this notebook). The sample_rate\n",
        "# parameter indicates what proportion of all possible pairs of observations\n",
        "# to use when searching for the optimal epsilon. If 1.0, all will be used. If\n",
        "# you have a very large number of pairs, you may want to lower this. We found\n",
        "# that small values (0.1) actually tend to yield relatively stable results\n",
        "# (see Appendix E in the paper).\n",
        "sample_rate = 1.0\n",
        "result = tau_optimization.tau_optimization(\n",
        "  X, Y, tau_optimization.TauSufficientStats.acc_23,\n",
        ")\n",
        "\n",
        "# The result object has various information, including `best_threshold` (equal\n",
        "# to the best epsilon), `best_tau` (equal to the best `acc_23` score), plus\n",
        "# `thresholds` and `taus` (they contain the various epsilsons and corresponding\n",
        "# accuracy scores that were used in the search).\n",
        "print(result.best_threshold)\n",
        "print(result.best_tau)\n",
        "\n",
        "# If you already have an epsilon and you want to compute an accuracy score\n",
        "# with that epsilon with two vectors, then you can do so with the following:\n",
        "x = np.random.rand(N)  # the metric scores\n",
        "y = np.random.rand(N)  # the human scores\n",
        "epsilon = 0.05\n",
        "accuracy, _ = stats.KendallVariants(\n",
        "  Y, X, variant=\"acc23\", epsilon=epsilon\n",
        ")\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0.61565245, 0.94124171, 0.78704148, 0.7048761 , 0.33322002],\n",
              "       [0.98010779, 0.51545494, 0.58234558, 0.45539542, 0.81932577]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ql2EuOTD3sfv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'mt-metrics-eval'...\n",
            "remote: Enumerating objects: 281, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 281 (delta 65), reused 56 (delta 38), pack-reused 177 (from 1)\u001b[K\n",
            "Receiving objects: 100% (281/281), 254.09 KiB | 7.26 MiB/s, done.\n",
            "Resolving deltas: 100% (171/171), done.\n",
            "Note: switching to 'd18c3ebe91a004c124c179ad5614b8dba96f1f48'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at d18c3eb Add dependency.\n",
            "Processing /Users/zhangran/Documents/GitHub/mt-metrics-eval/mt-metrics-eval\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: apache_beam in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from mt-metrics-eval==0.0.2) (2.63.0)\n",
            "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from mt-metrics-eval==0.0.2) (1.24.2)\n",
            "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from mt-metrics-eval==0.0.2) (1.10.1)\n",
            "Requirement already satisfied: absl-py in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from mt-metrics-eval==0.0.2) (2.2.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (1.7)\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (3.10.15)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (0.3.1.1)\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (1.10.0)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (0.19)\n",
            "Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (1.65.5)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (2.7.3)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (4.23.0)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (3.4.2)\n",
            "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (0.7.1)\n",
            "Requirement already satisfied: packaging>=22.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (24.2)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (4.11.3)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<6.0.0.dev0,>=3.20.3 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (5.29.4)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2018.3 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (2022.7.1)\n",
            "Requirement already satisfied: redis<6,>=5.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (5.2.1)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (2024.11.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (2.28.2)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (4.4.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (0.23.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (6.0)\n",
            "Requirement already satisfied: pyarrow<17.0.0,>=3.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from apache_beam->mt-metrics-eval==0.0.2) (0.6)\n",
            "Requirement already satisfied: docopt in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam->mt-metrics-eval==0.0.2) (0.6.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam->mt-metrics-eval==0.0.2) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from httplib2<0.23.0,>=0.8->apache_beam->mt-metrics-eval==0.0.2) (3.2.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam->mt-metrics-eval==0.0.2) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam->mt-metrics-eval==0.0.2) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam->mt-metrics-eval==0.0.2) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam->mt-metrics-eval==0.0.2) (0.23.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from pymongo<5.0.0,>=3.8.0->apache_beam->mt-metrics-eval==0.0.2) (2.7.0)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from redis<6,>=5.0.0->apache_beam->mt-metrics-eval==0.0.2) (5.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->apache_beam->mt-metrics-eval==0.0.2) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->apache_beam->mt-metrics-eval==0.0.2) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->apache_beam->mt-metrics-eval==0.0.2) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/metric/lib/python3.11/site-packages (from requests<3.0.0,>=2.24.0->apache_beam->mt-metrics-eval==0.0.2) (2025.1.31)\n",
            "Building wheels for collected packages: mt-metrics-eval\n",
            "  Building wheel for mt-metrics-eval (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for mt-metrics-eval: filename=mt_metrics_eval-0.0.2-py3-none-any.whl size=60074 sha256=9d97dc3c31c230a4127ff3e9fbbdbfb24a65e09474e8437b29adbd753b354dbb\n",
            "  Stored in directory: /Users/zhangran/Library/Caches/pip/wheels/62/2a/6e/0cec567d5476acbe633797c928faa603f7afb7ea15b724ce53\n",
            "Successfully built mt-metrics-eval\n",
            "Installing collected packages: mt-metrics-eval\n",
            "  Attempting uninstall: mt-metrics-eval\n",
            "    Found existing installation: mt-metrics-eval 0.0.3\n",
            "    Uninstalling mt-metrics-eval-0.0.3:\n",
            "      Successfully uninstalled mt-metrics-eval-0.0.3\n",
            "Successfully installed mt-metrics-eval-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/google-research/mt-metrics-eval.git && cd mt-metrics-eval && git checkout d18c3ebe91a004c124c179ad5614b8dba96f1f48 && pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0msK-QWFGUU"
      },
      "source": [
        "## Download Data\n",
        "Downloads the WMT'22 metrics scores and the GEMBA metric outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YP5btM7746c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data into /Users/zhangran/.mt-metrics-eval\n",
            "--2025-03-21 17:35:00--  https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-de/GEMBA-Dav3-DA-refA.seg.score\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 489316 (478K) [text/plain]\n",
            "Saving to: ‘gemba/wmt22/metric-scores/en-de/GEMBA-Dav3-DA-refA.seg.score’\n",
            "\n",
            "gemba/wmt22/metric- 100%[===================>] 477,85K  --.-KB/s    in 0,05s   \n",
            "\n",
            "2025-03-21 17:35:01 (10,4 MB/s) - ‘gemba/wmt22/metric-scores/en-de/GEMBA-Dav3-DA-refA.seg.score’ saved [489316/489316]\n",
            "\n",
            "--2025-03-21 17:35:01--  https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-ru/GEMBA-Dav3-DA-refA.seg.score\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 491044 (480K) [text/plain]\n",
            "Saving to: ‘gemba/wmt22/metric-scores/en-ru/GEMBA-Dav3-DA-refA.seg.score’\n",
            "\n",
            "gemba/wmt22/metric- 100%[===================>] 479,54K  --.-KB/s    in 0,04s   \n",
            "\n",
            "2025-03-21 17:35:02 (10,7 MB/s) - ‘gemba/wmt22/metric-scores/en-ru/GEMBA-Dav3-DA-refA.seg.score’ saved [491044/491044]\n",
            "\n",
            "--2025-03-21 17:35:02--  https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/zh-en/GEMBA-Dav3-DA-refA.seg.score\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 523799 (512K) [text/plain]\n",
            "Saving to: ‘gemba/wmt22/metric-scores/zh-en/GEMBA-Dav3-DA-refA.seg.score’\n",
            "\n",
            "gemba/wmt22/metric- 100%[===================>] 511,52K  --.-KB/s    in 0,05s   \n",
            "\n",
            "2025-03-21 17:35:02 (10,2 MB/s) - ‘gemba/wmt22/metric-scores/zh-en/GEMBA-Dav3-DA-refA.seg.score’ saved [523799/523799]\n",
            "\n",
            "--2025-03-21 17:35:03--  https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-de/GEMBA-GPT4-DA-refA.seg.score\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 490028 (479K) [text/plain]\n",
            "Saving to: ‘gemba/wmt22/metric-scores/en-de/GEMBA-GPT4-DA-refA.seg.score’\n",
            "\n",
            "gemba/wmt22/metric- 100%[===================>] 478,54K  --.-KB/s    in 0,04s   \n",
            "\n",
            "2025-03-21 17:35:03 (10,4 MB/s) - ‘gemba/wmt22/metric-scores/en-de/GEMBA-GPT4-DA-refA.seg.score’ saved [490028/490028]\n",
            "\n",
            "--2025-03-21 17:35:03--  https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-ru/GEMBA-GPT4-DA-refA.seg.score\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 492219 (481K) [text/plain]\n",
            "Saving to: ‘gemba/wmt22/metric-scores/en-ru/GEMBA-GPT4-DA-refA.seg.score’\n",
            "\n",
            "gemba/wmt22/metric- 100%[===================>] 480,68K  --.-KB/s    in 0,05s   \n",
            "\n",
            "2025-03-21 17:35:04 (10,4 MB/s) - ‘gemba/wmt22/metric-scores/en-ru/GEMBA-GPT4-DA-refA.seg.score’ saved [492219/492219]\n",
            "\n",
            "--2025-03-21 17:35:04--  https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/zh-en/GEMBA-GPT4-DA-refA.seg.score\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 523798 (512K) [text/plain]\n",
            "Saving to: ‘gemba/wmt22/metric-scores/zh-en/GEMBA-GPT4-DA-refA.seg.score’\n",
            "\n",
            "gemba/wmt22/metric- 100%[===================>] 511,52K  --.-KB/s    in 0,05s   \n",
            "\n",
            "2025-03-21 17:35:04 (10,7 MB/s) - ‘gemba/wmt22/metric-scores/zh-en/GEMBA-GPT4-DA-refA.seg.score’ saved [523798/523798]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# MTME data\n",
        "!python3 -m mt_metrics_eval.mtme --download\n",
        "\n",
        "# GEMBA data\n",
        "!mkdir -p gemba/wmt22/metric-scores/en-de gemba/wmt22/metric-scores/en-ru gemba/wmt22/metric-scores/zh-en\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-de/GEMBA-Dav3-DA-refA.seg.score -O gemba/wmt22/metric-scores/en-de/GEMBA-Dav3-DA-refA.seg.score\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-ru/GEMBA-Dav3-DA-refA.seg.score -O gemba/wmt22/metric-scores/en-ru/GEMBA-Dav3-DA-refA.seg.score\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/zh-en/GEMBA-Dav3-DA-refA.seg.score -O gemba/wmt22/metric-scores/zh-en/GEMBA-Dav3-DA-refA.seg.score\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-de/GEMBA-GPT4-DA-refA.seg.score -O gemba/wmt22/metric-scores/en-de/GEMBA-GPT4-DA-refA.seg.score\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-ru/GEMBA-GPT4-DA-refA.seg.score -O gemba/wmt22/metric-scores/en-ru/GEMBA-GPT4-DA-refA.seg.score\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/zh-en/GEMBA-GPT4-DA-refA.seg.score -O gemba/wmt22/metric-scores/zh-en/GEMBA-GPT4-DA-refA.seg.score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o-PgbRNFNM9"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N7wlbEG839na"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.patches import Patch\n",
        "from mt_metrics_eval import data as mtme_data\n",
        "from mt_metrics_eval import stats as mtme_stats\n",
        "from mt_metrics_eval import tau_optimization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats\n",
        "from typing import Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMJAyHmSFOvb"
      },
      "source": [
        "## Load WMT'22 Evaluation Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IJmSbDP44gUv"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'gemba/wmt22/documents/en-de.docs'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m eval_sets \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-de\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mmtme_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEvalSet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwmt22\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men-de\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_stored_metric_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemba\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/gemba/wmt22/metric-scores\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-ru\u001b[39m\u001b[38;5;124m\"\u001b[39m: mtme_data\u001b[38;5;241m.\u001b[39mEvalSet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwmt22\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men-ru\u001b[39m\u001b[38;5;124m\"\u001b[39m, read_stored_metric_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, path\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemba\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/gemba/wmt22/metric-scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzh-en\u001b[39m\u001b[38;5;124m\"\u001b[39m: mtme_data\u001b[38;5;241m.\u001b[39mEvalSet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwmt22\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzh-en\u001b[39m\u001b[38;5;124m\"\u001b[39m, read_stored_metric_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, path\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemba\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/gemba/wmt22/metric-scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m      5\u001b[0m }\n",
            "File \u001b[0;32m~/Documents/GitHub/mt-metrics-eval/mt_metrics_eval/data.py:96\u001b[0m, in \u001b[0;36mEvalSet.__init__\u001b[0;34m(self, name, lp, read_stored_metric_scores, info, path, strict, read_stored_ratings)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_std_human_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mstd_gold\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mprimary_metrics\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ReadDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_stored_metric_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_stored_ratings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Check compatibility between info and data read in.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# No checks for primary metrics because there are no hard requirements:\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# no metrics for this lp need to be primary.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstd_ref \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_names:\n",
            "File \u001b[0;32m~/Documents/GitHub/mt-metrics-eval/mt_metrics_eval/data.py:534\u001b[0m, in \u001b[0;36mEvalSet._ReadDataset\u001b[0;34m(self, name, lp, read_stored_metric_scores, path, strict, read_stored_ratings)\u001b[0m\n\u001b[1;32m    531\u001b[0m   metric_scores_paths \u001b[38;5;241m=\u001b[39m [path]\n\u001b[1;32m    533\u001b[0m d \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, name)\n\u001b[0;32m--> 534\u001b[0m doc_lines \u001b[38;5;241m=\u001b[39m \u001b[43m_ReadTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocuments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m.docs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_domains \u001b[38;5;241m=\u001b[39m _MapPositions([d\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m doc_lines])\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# Canonicalized domain order, since there is no natural order.\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/GitHub/mt-metrics-eval/mt_metrics_eval/data.py:661\u001b[0m, in \u001b[0;36m_ReadTextFile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_ReadTextFile\u001b[39m(filename):\n\u001b[0;32m--> 661\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    662\u001b[0m     lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gemba/wmt22/documents/en-de.docs'"
          ]
        }
      ],
      "source": [
        "eval_sets = {\n",
        "    \"en-de\": mtme_data.EvalSet(\"wmt22\", \"en-de\", read_stored_metric_scores=True, path=[\"gemba\", \"/gemba/wmt22/metric-scores\"]),\n",
        "    \"en-ru\": mtme_data.EvalSet(\"wmt22\", \"en-ru\", read_stored_metric_scores=True, path=[\"gemba\", \"/gemba/wmt22/metric-scores\"]),\n",
        "    \"zh-en\": mtme_data.EvalSet(\"wmt22\", \"zh-en\", read_stored_metric_scores=True, path=[\"gemba\", \"/gemba/wmt22/metric-scores\"]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjlymCXAFUQy"
      },
      "source": [
        "## Utility Functions\n",
        "Implements getting and filtering scores, calculating correlations that aren't checked in to the MTME library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6TdWE4F6wRo"
      },
      "outputs": [],
      "source": [
        "def get_metric_scores(evs: mtme_data.EvalSet, metric: str) -> dict[str, list[float]]:\n",
        "  scores_dict = evs.Scores(\"seg\", metric)\n",
        "  bad_systems = evs.outlier_sys_names | {evs.std_ref}\n",
        "  return {\n",
        "      system: scores for system, scores in scores_dict.items() if system not in bad_systems\n",
        "  }\n",
        "\n",
        "\n",
        "def kendall(x, y, variant):\n",
        "  x = np.asarray(x).ravel()\n",
        "  y = np.asarray(y).ravel()\n",
        "\n",
        "  if x.size != y.size:\n",
        "    raise ValueError(\n",
        "        'All inputs to `kendalltau` must be of the same '\n",
        "        f'size, found x-size {x.size} and y-size {y.size}'\n",
        "    )\n",
        "  elif not x.size or not y.size:\n",
        "    raise ValueError('x or y are empty')\n",
        "\n",
        "  # check both x and y\n",
        "  cnx = np.any(np.isnan(x))\n",
        "  cny = np.any(np.isnan(y))\n",
        "  contains_nan = cnx or cny\n",
        "  if contains_nan:\n",
        "    raise ValueError('x or y contains NaN')\n",
        "\n",
        "  def count_rank_tie(ranks):\n",
        "    cnt = np.bincount(ranks).astype('int64', copy=False)\n",
        "    cnt = cnt[cnt > 1]\n",
        "    return (\n",
        "        (cnt * (cnt - 1) // 2).sum(),\n",
        "        (cnt * (cnt - 1.0) * (cnt - 2)).sum(),\n",
        "        (cnt * (cnt - 1.0) * (2 * cnt + 5)).sum(),\n",
        "    )\n",
        "\n",
        "  size = x.size\n",
        "  perm = np.argsort(y)  # sort on y and convert y to dense ranks\n",
        "  x, y = x[perm], y[perm]\n",
        "  y = np.r_[True, y[1:] != y[:-1]].cumsum(dtype=np.intp)\n",
        "\n",
        "  # stable sort on x and convert x to dense ranks\n",
        "  perm = np.argsort(x, kind='mergesort')\n",
        "  x, y = x[perm], y[perm]\n",
        "  x = np.r_[True, x[1:] != x[:-1]].cumsum(dtype=np.intp)\n",
        "\n",
        "  dis = scipy.stats._stats._kendall_dis(x, y)  # discordant pairs\n",
        "\n",
        "  obs = np.r_[True, (x[1:] != x[:-1]) | (y[1:] != y[:-1]), True]\n",
        "  cnt = np.diff(np.nonzero(obs)[0]).astype('int64', copy=False)\n",
        "\n",
        "  ntie = (cnt * (cnt - 1) // 2).sum()  # joint ties\n",
        "  xtie, _, _ = count_rank_tie(x)  # ties in x, stats\n",
        "  ytie, _, _ = count_rank_tie(y)  # ties in y, stats\n",
        "\n",
        "  tot = (size * (size - 1)) // 2\n",
        "  con = tot - ((xtie - ntie) + (ytie - ntie) + ntie + dis)\n",
        "\n",
        "  minclasses = min(len(set(x)), len(set(y)))\n",
        "\n",
        "  tx = xtie - ntie\n",
        "  ty = ytie - ntie\n",
        "  txy = ntie\n",
        "\n",
        "  if variant == \"a\":\n",
        "    return (con - dis) / (con + dis + tx + ty + txy), 0\n",
        "  elif variant == \"b\":\n",
        "    return (con - dis) / np.sqrt((con + dis + tx) * (con + dis + ty)), 0\n",
        "  elif variant == \"c\":\n",
        "    minclasses = min(len(set(x)), len(set(y)))\n",
        "    return 2 * (con - dis) / (size**2 * (minclasses - 1) / minclasses), 0\n",
        "  elif variant == \"10\":\n",
        "    return (con - dis - ty) / (con + dis + ty), 0\n",
        "  elif variant == \"13\":\n",
        "    return (con - dis) / (con + dis), 0\n",
        "  elif variant == \"14\":\n",
        "    return (con - dis) / (con + dis + ty), 0\n",
        "  elif variant == \"acc_eq\":\n",
        "    # Accuracy assuming tie optimization is done.\n",
        "    return (con + txy) / (con + dis + tx + ty + txy), 0\n",
        "  return 0\n",
        "\n",
        "\n",
        "def custom_kendall(corr, variant: str, average_by: str = \"none\"):\n",
        "  cf = corr.AverageCorrelation(\n",
        "      kendall, average_by, variant=variant)\n",
        "  return cf(corr.gold_scores, corr.metric_scores)\n",
        "\n",
        "\n",
        "def calculate_correlations(\n",
        "    evs: mtme_data.EvalSet,\n",
        "    mqm_scores: dict[str, list[float]],\n",
        "    metric_scores: dict[str, list[float]],\n",
        "    coef: str,\n",
        ") -> dict[str, float]:\n",
        "  corr = evs.Correlation(mqm_scores, metric_scores)\n",
        "  if coef == \"kendall-a\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"a\")\n",
        "  elif coef == \"kendall-b\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"b\")\n",
        "  elif coef == \"kendall-c\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"c\")\n",
        "  elif coef == \"kendall-10\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"10\")\n",
        "  elif coef == \"kendall-13\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"13\")\n",
        "  elif coef == \"kendall-14\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"14\")\n",
        "  elif coef == \"accuracy-eq_no_calib\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"acc_eq\")\n",
        "  elif coef == \"accuracy-eq\":\n",
        "    corr_fn = functools.partial(\n",
        "        corr.KendallWithTiesOpt,\n",
        "        sample_rate=1.0\n",
        "    )\n",
        "  elif coef == \"pearson\":\n",
        "    corr_fn = corr.Pearson\n",
        "  else:\n",
        "    raise ValueError(coef)\n",
        "\n",
        "  no_grouping = corr_fn()[0]\n",
        "  group_by_item, _, num_items = corr_fn(average_by=\"item\")\n",
        "  group_by_system = corr_fn(average_by=\"sys\")[0]\n",
        "  return {\n",
        "      \"no_grouping\": no_grouping,\n",
        "      \"group_by_item\": group_by_item,\n",
        "      \"group_by_item_num_items\": num_items,\n",
        "      \"group_by_system\": group_by_system,\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pR69U_pFeCl"
      },
      "source": [
        "## Analyze Ties\n",
        "These results correspond to Tables 3, 4, and 10 from the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9icczg96GzO"
      },
      "outputs": [],
      "source": [
        "def analyze_ties(grouping: str, metric: str) -> pd.DataFrame:\n",
        "  df = []\n",
        "  for lp, evs in eval_sets.items():\n",
        "    mqm_dict = get_metric_scores(evs, \"mqm\")\n",
        "    scores_dict = get_metric_scores(evs, metric)\n",
        "    num_translations = 0\n",
        "    num_pairs = 0\n",
        "    num_tied_pairs = 0\n",
        "    num_zero_pairs = 0\n",
        "\n",
        "    if grouping == \"no_grouping\":\n",
        "      all_scores = []\n",
        "      for system, scores in scores_dict.items():\n",
        "        for i, score in enumerate(scores):\n",
        "          if score is not None and mqm_dict[system][i] is not None:\n",
        "            all_scores.append(score)\n",
        "\n",
        "      num_translations = len(all_scores)\n",
        "      for i in range(len(all_scores)):\n",
        "        for j in range(i + 1, len(all_scores)):\n",
        "          num_pairs += 1\n",
        "          if all_scores[i] == all_scores[j]:\n",
        "            num_tied_pairs += 1\n",
        "            if all_scores[i] == 0.0:\n",
        "              num_zero_pairs += 1\n",
        "\n",
        "    elif grouping == \"group_by_item\":\n",
        "      for i in range(len(evs.src)):\n",
        "        item_scores = []\n",
        "        for system, scores in scores_dict.items():\n",
        "          if scores[i] is not None and mqm_dict[system][i] is not None:\n",
        "            item_scores.append(scores[i])\n",
        "\n",
        "        num_translations += len(item_scores)\n",
        "        for j in range(len(item_scores)):\n",
        "          for k in range(j + 1, len(item_scores)):\n",
        "            num_pairs += 1\n",
        "            if item_scores[j] == item_scores[k]:\n",
        "              num_tied_pairs += 1\n",
        "              if item_scores[j] == 0.0:\n",
        "                num_zero_pairs += 1\n",
        "\n",
        "    elif grouping == \"group_by_system\":\n",
        "      for system, scores in scores_dict.items():\n",
        "        system_scores = []\n",
        "        for i, score in enumerate(scores):\n",
        "          if score is not None and mqm_dict[system][i] is not None:\n",
        "            system_scores.append(score)\n",
        "\n",
        "        num_translations += len(system_scores)\n",
        "        for i in range(len(system_scores)):\n",
        "          for j in range(i + 1, len(system_scores)):\n",
        "            num_pairs += 1\n",
        "            if system_scores[i] == system_scores[j]:\n",
        "              num_tied_pairs += 1\n",
        "              if system_scores[i] == 0.0:\n",
        "                num_zero_pairs += 1\n",
        "\n",
        "    df.append({\n",
        "        \"lp\": lp,\n",
        "        \"num_translations\": num_translations,\n",
        "        \"num_pairs\": num_pairs,\n",
        "        \"num_tied_pairs\": num_tied_pairs,\n",
        "        \"percent_of_pairs_tied\": num_tied_pairs / num_pairs * 100,\n",
        "        \"num_zero_pairs\": num_zero_pairs,\n",
        "        \"percent_of_tied_pairs_zero_tied\": num_zero_pairs / num_tied_pairs * 100,\n",
        "        \"percent_of_pairs_zero_tied\": num_zero_pairs / num_pairs * 100,\n",
        "    })\n",
        "  return pd.DataFrame(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hFLAvOvFh6Q"
      },
      "source": [
        "### MQM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH5TK1NE8mYl"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"no_grouping\", \"mqm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wrjUYMR7fEu"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"mqm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQMmc2-F7gjh"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_system\", \"mqm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcI9o5g-Fl4Q"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lB7QrD09P67"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"metricx_xxl_MQM_2020-refA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp3N_rr--vJM"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"COMET-22-refA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLu--Vx--v-5"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"MATESE-refA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtbV8xsA-2G-"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"GEMBA-Dav3-DA-refA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJgQzf2w-2tr"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"GEMBA-GPT4-DA-refA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXspo9BMFonV"
      },
      "source": [
        "## Equal Width Buckets Experiments\n",
        "This experiment maps a metric score into k buckets of each width, simulating what would happen if the metric predicted a larger number of ties than it actually does.\n",
        "This experiment produces Figure 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nju81PpA0xy"
      },
      "outputs": [],
      "source": [
        "def map_to_equal_width_buckets(\n",
        "    evs: mtme_data.EvalSet,\n",
        "    scores_dict: dict[str, list[float]],\n",
        "    num_buckets: int,\n",
        ") -> dict[str, list[float]]:\n",
        "  \"\"\"Maps the scores to integer buckets where each bucket represents an equal score range.\"\"\"\n",
        "  all_scores = [x for scores in scores_dict.values() for x in scores]\n",
        "  min_value = min(all_scores)\n",
        "  max_value = max(all_scores)\n",
        "  width = (max_value - min_value) / num_buckets\n",
        "  bins = [min_value + width * (i + 1) for i in range(num_buckets - 1)]\n",
        "  return {\n",
        "      system: np.digitize(scores, bins)  for system, scores in scores_dict.items()\n",
        "  }\n",
        "\n",
        "def run_equal_width_buckets_experiment(lp: str, metric: str, coef: str):\n",
        "  evs = eval_sets[lp]\n",
        "  num_buckets_list = [\n",
        "      2, 3, 4, 5, 10, 25, 50, 100, 200, 500, 1000, 5000, 10000, 20000, 30000\n",
        "  ]\n",
        "  mqm_dict = get_metric_scores(evs, \"mqm\")\n",
        "  scores_dict = get_metric_scores(evs, metric)\n",
        "\n",
        "  # Calculate the correlation when the scores are bucketed\n",
        "  correlations = []\n",
        "  num_non_nan_segments = []\n",
        "  for num_buckets in num_buckets_list:\n",
        "    bucketed_scores = map_to_equal_width_buckets(evs, scores_dict, num_buckets)\n",
        "    correlations_dict = calculate_correlations(evs, mqm_dict, bucketed_scores, coef)\n",
        "    correlations.append(correlations_dict[\"group_by_item\"])\n",
        "    num_non_nan_segments.append(correlations_dict[\"group_by_item_num_items\"])\n",
        "\n",
        "  # Calculate the original correlation\n",
        "  original_dict = calculate_correlations(evs, mqm_dict, scores_dict, coef)\n",
        "  original = original_dict[\"group_by_item\"]\n",
        "\n",
        "  return {\n",
        "      \"num_buckets\": num_buckets_list,\n",
        "      \"bucketed_correlations\": correlations,\n",
        "      \"num_segments\": num_non_nan_segments,\n",
        "      \"original_correlation\": original,\n",
        "      \"original_correlation_num_segments\": original_dict[\"group_by_item_num_items\"],\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kokqulivml8T"
      },
      "outputs": [],
      "source": [
        "def plot_bucketed_correlations(\n",
        "    lp: str,\n",
        "    metric: str,\n",
        "    data: dict[str, Any],\n",
        "):\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n",
        "\n",
        "  num_buckets = data[\"num_buckets\"]\n",
        "  x = np.log10(num_buckets)\n",
        "\n",
        "  y1_bucketed = data[\"bucketed_correlations\"]\n",
        "  y1_original = data[\"original_correlation\"]\n",
        "\n",
        "  y2_bucketed = data[\"num_segments\"]\n",
        "  y2_original = data[\"original_correlation_num_segments\"]\n",
        "\n",
        "  axes[0].plot(x, y1_bucketed, label=\"Bucketed Scores\", color=\"blue\", marker=\"o\", markersize=10)\n",
        "  axes[0].axhline(y1_original, label=\"Original Scores\", linestyle=\"dashed\", color=\"orange\")\n",
        "\n",
        "  axes[1].plot(x, y2_bucketed, label=\"Bucketed Scores\", color=\"blue\", marker=\"o\", markersize=10)\n",
        "  axes[1].axhline(y2_original, label=\"Original Scores\", linestyle=\"dashed\", color=\"orange\")\n",
        "\n",
        "  axes[0].set_ylabel(\"Group-by-Item $\\\\tau_b$\")\n",
        "  axes[1].set_xlabel(\"log$_{10}$(Number of Buckets)\")\n",
        "  axes[1].set_ylabel(\"#Non-NaN Groups\")\n",
        "\n",
        "  handles, labels = axes[0].get_legend_handles_labels()\n",
        "\n",
        "  # Add legend with the modified handles and labels. Disable the legend box,\n",
        "  # which only takes up space and doesn't look good anyway.\n",
        "  # Reshape the line symbol a bit.\n",
        "  axes[0].legend(handles, labels, frameon=False, handlelength=2.2,\n",
        "                 handletextpad=0.5, borderpad=1)\n",
        "\n",
        "  # Modify space beween subplots\n",
        "  fig.subplots_adjust(hspace=0.2)\n",
        "\n",
        "  fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnYaFY86nIZ_"
      },
      "outputs": [],
      "source": [
        "lp = \"en-de\"\n",
        "metric = \"metricx_xxl_MQM_2020-refA\"\n",
        "results = run_equal_width_buckets_experiment(lp, metric, \"kendall-b\")\n",
        "plot_bucketed_correlations(lp, metric, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLuJp9OoF8vH"
      },
      "source": [
        "## Tie Calibration Experiments\n",
        "This reproduces the main result of the paper.\n",
        "It ranks metrics by different correlation coefficients, including the pairwise accuracy with tie calibration.\n",
        "These results correspond to Tables 6 and 12-20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umlX7yCdperh"
      },
      "outputs": [],
      "source": [
        "def select_optimal_epsilon(\n",
        "    evs: mtme_data.EvalSet,\n",
        "    metric: str,\n",
        "    grouping: str,\n",
        "):\n",
        "  mqm_scores = get_metric_scores(evs, \"mqm\")\n",
        "  metric_scores = get_metric_scores(evs, metric)\n",
        "  if not metric_scores:\n",
        "    return None\n",
        "\n",
        "  if grouping == \"no_grouping\":\n",
        "    average_by = \"none\"\n",
        "    sample_rate = 0.1\n",
        "  elif grouping == \"group_by_item\":\n",
        "    average_by = \"item\"\n",
        "    sample_rate = 1.0\n",
        "  elif grouping == \"group_by_system\":\n",
        "    average_by = \"sys\"\n",
        "    sample_rate = 0.1\n",
        "  else:\n",
        "    raise ValueError(grouping)\n",
        "\n",
        "  corr = evs.Correlation(mqm_scores, metric_scores)\n",
        "\n",
        "  return mtme_stats.KendallWithTiesOpt(\n",
        "      corr.gold_scores,\n",
        "      corr.metric_scores,\n",
        "      num_sys=corr.num_sys,\n",
        "      average_by=average_by,\n",
        "      sample_rate=sample_rate,\n",
        "  )\n",
        "\n",
        "\n",
        "def run_tie_calibration_experiment(\n",
        "    evs: mtme_data.EvalSet,\n",
        "    grouping: str,\n",
        "    metrics: list[str],\n",
        "):\n",
        "  mqm_scores = get_metric_scores(evs, \"mqm\")\n",
        "  coefs = [\"kendall-a\", \"kendall-b\", \"kendall-c\", \"kendall-10\", \"kendall-13\", \"kendall-14\", \"accuracy-eq_no_calib\"]\n",
        "\n",
        "  df = []\n",
        "  for metric in metrics:\n",
        "    # Run the optimization\n",
        "    opt_results = select_optimal_epsilon(\n",
        "        evs, metric, grouping,\n",
        "    )\n",
        "    if not opt_results:\n",
        "      continue\n",
        "\n",
        "    # Calculate baseline metric scores\n",
        "    metric_scores = get_metric_scores(evs, metric)\n",
        "    correlations = {\n",
        "        coef: calculate_correlations(\n",
        "            evs,\n",
        "            mqm_scores,\n",
        "            metric_scores,\n",
        "            coef,\n",
        "        ) for coef in coefs\n",
        "    }\n",
        "\n",
        "    results = {\n",
        "        \"metric\": metric,\n",
        "        \"best_epsilon\": opt_results[1],\n",
        "        \"best_acc\": opt_results[0],\n",
        "    }\n",
        "    for coef in coefs:\n",
        "      results[coef] = correlations[coef][grouping]\n",
        "    df.append(results)\n",
        "\n",
        "  df = pd.DataFrame(df)\n",
        "  for coef in [\"best_acc\"] + coefs:\n",
        "    df[f\"{coef}_rank\"] = df[coef].rank(ascending=False)\n",
        "\n",
        "  return df.sort_values(by=[\"best_acc_rank\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl5Qb4WgEukK"
      },
      "outputs": [],
      "source": [
        "ende_metrics = [\n",
        "    \"metricx_xxl_MQM_2020-refA\",\n",
        "    \"UniTE-ref-refA\",\n",
        "    \"COMET-22-refA\",\n",
        "    \"MATESE-refA\",\n",
        "    \"UniTE-src-src\",\n",
        "    \"GEMBA-GPT4-DA-refA\",\n",
        "    \"MATESE-QE-src\",\n",
        "    \"COMETKiwi-src\",\n",
        "    \"MS-COMET-22-refA\",\n",
        "    \"COMET-QE-src\",\n",
        "    \"SEScore-refA\",\n",
        "    \"HWTSC-Teacher-Sim-src\",\n",
        "    \"GEMBA-Dav3-DA-refA\",\n",
        "    \"MEE-refA\",\n",
        "    \"REUSE-src\",\n",
        "    \"BLEURT-20-refA\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTZYtaEDD4FT"
      },
      "outputs": [],
      "source": [
        "# Too slow to run on the free Colab server\n",
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"en-de\"],\n",
        "#     \"no_grouping\",\n",
        "#     ende_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFr6nDDlzD_T"
      },
      "outputs": [],
      "source": [
        "run_tie_calibration_experiment(\n",
        "    eval_sets[\"en-de\"],\n",
        "    \"group_by_item\",\n",
        "    ende_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4-w-vjOD5av"
      },
      "outputs": [],
      "source": [
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"en-de\"],\n",
        "#     \"group_by_system\",\n",
        "#     ende_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H3y_k_7rpWh"
      },
      "outputs": [],
      "source": [
        "enru_metrics = [\n",
        "    \"metricx_xxl_MQM_2020-refA\",\n",
        "    \"UniTE-ref-refA\",\n",
        "    \"COMET-22-refA\",\n",
        "    \"MATESE-refA\",\n",
        "    \"UniTE-src-src\",\n",
        "    \"GEMBA-GPT4-DA-refA\",\n",
        "    \"MATESE-QE-src\",\n",
        "    \"COMETKiwi-src\",\n",
        "    \"MS-COMET-22-refA\",\n",
        "    \"COMET-QE-src\",\n",
        "    \"HWTSC-Teacher-Sim-src\",\n",
        "    \"GEMBA-Dav3-DA-refA\",\n",
        "    \"MEE-refA\",\n",
        "    \"REUSE-src\",\n",
        "    \"BLEURT-20-refA\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udGSQw01r3q3"
      },
      "outputs": [],
      "source": [
        "# Too slow/too much memory for public Colab\n",
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"en-ru\"],\n",
        "#     \"no_grouping\",\n",
        "#     enru_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQDGZJTUrWFM"
      },
      "outputs": [],
      "source": [
        "run_tie_calibration_experiment(\n",
        "    eval_sets[\"en-ru\"],\n",
        "    \"group_by_item\",\n",
        "    enru_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVnFKLxrr4Xm"
      },
      "outputs": [],
      "source": [
        "# Too slow/too much memory for public Colab\n",
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"en-ru\"],\n",
        "#     \"group_by_system\",\n",
        "#     enru_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEqF230Lro8_"
      },
      "outputs": [],
      "source": [
        "zhen_metrics = [\n",
        "    \"metricx_xxl_MQM_2020-refA\",\n",
        "    \"UniTE-ref-refA\",\n",
        "    \"COMET-22-refA\",\n",
        "    \"MATESE-refA\",\n",
        "    \"UniTE-src-src\",\n",
        "    \"GEMBA-GPT4-DA-refA\",\n",
        "    \"MATESE-QE-src\",\n",
        "    \"COMETKiwi-src\",\n",
        "    \"MS-COMET-22-refA\",\n",
        "    \"COMET-QE-src\",\n",
        "    \"SEScore-refA\",\n",
        "    \"HWTSC-Teacher-Sim-src\",\n",
        "    \"GEMBA-Dav3-DA-refA\",\n",
        "    \"MEE-refA\",\n",
        "    \"REUSE-src\",\n",
        "    \"BLEURT-20-refA\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrM6nfU5rwEV"
      },
      "outputs": [],
      "source": [
        "# Too slow/too much memory for public Colab\n",
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"zh-en\"],\n",
        "#     \"no_grouping\",\n",
        "#     zhen_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xvVZt--rWBO"
      },
      "outputs": [],
      "source": [
        "run_tie_calibration_experiment(\n",
        "    eval_sets[\"zh-en\"],\n",
        "    \"group_by_item\",\n",
        "    zhen_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_spZUfBirwix"
      },
      "outputs": [],
      "source": [
        "# Too slow/too much memory for public Colab\n",
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"zh-en\"],\n",
        "#     \"group_by_system\",\n",
        "#     zhen_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPId69h0GkdT"
      },
      "source": [
        "## Analyze Epsilon Across Years\n",
        "This experiment analyzes whether or not the epsilon selected from one year of WMT generalizes to other years.\n",
        "This corresponds to Figure 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH4fHVTm-pIc"
      },
      "outputs": [],
      "source": [
        "def convert_to_matrices(mqm_scores: dict[str, list[float]], metric_scores: dict[str, list[float]]):\n",
        "  X, Y = [], []\n",
        "  for system in mqm_scores.keys():\n",
        "    if system not in metric_scores:\n",
        "      continue\n",
        "    x = metric_scores[system]\n",
        "    y = mqm_scores[system]\n",
        "    if not y or not any(score is not None for score in y):\n",
        "      continue\n",
        "    assert len(x) == len(y)\n",
        "    X.append([x[i] for i in range(len(x)) if y[i] is not None])\n",
        "    Y.append([y[i] for i in range(len(y)) if y[i] is not None])\n",
        "  return np.array(X), np.array(Y)\n",
        "\n",
        "\n",
        "def calculate_group_by_item_acc(X: np.ndarray, Y: np.ndarray, epsilon: float):\n",
        "  accs = []\n",
        "  for x, y in zip(X.T, Y.T):\n",
        "    accs.append(mtme_stats.KendallVariants(x, y, variant=\"acc23\", epsilon=epsilon)[0])\n",
        "  return np.mean(accs)\n",
        "\n",
        "\n",
        "def _compare_epsilson_across_years(lp: str, metric: str, ax, legend: bool = True, xlabel = True, ylabel = True):\n",
        "  if lp == \"en-de\":\n",
        "    if metric == \"bleurt\":\n",
        "      metric21 = \"bleurt-20-refC\"\n",
        "      metric22 = \"BLEURT-20-refA\"\n",
        "    elif metric == \"comet\":\n",
        "      metric21 = \"COMET-DA_2020-refC\"\n",
        "      metric22 = \"COMET-20-refA\"\n",
        "    elif metric == \"bleu\":\n",
        "      metric21 = \"sentBLEU-refC\"\n",
        "      metric22 = \"BLEU-refA\"\n",
        "    elif metric == \"bertscore\":\n",
        "      metric21 = \"BERTScore-refC\"\n",
        "      metric22 = \"BERTScore-refA\"\n",
        "  elif lp == \"zh-en\":\n",
        "    if metric == \"bleurt\":\n",
        "      metric21 = \"bleurt-20-refB\"\n",
        "      metric22 = \"BLEURT-20-refA\"\n",
        "    elif metric == \"comet\":\n",
        "      metric21 = \"COMET-DA_2020-refB\"\n",
        "      metric22 = \"COMET-20-refA\"\n",
        "    elif metric == \"bleu\":\n",
        "      metric21 = \"sentBLEU-refB\"\n",
        "      metric22 = \"BLEU-refA\"\n",
        "    elif metric == \"bertscore\":\n",
        "      metric21 = \"BERTScore-refB\"\n",
        "      metric22 = \"BERTScore-refA\"\n",
        "\n",
        "  evs21 = mtme_data.EvalSet(\"wmt21.news\", lp, read_stored_metric_scores=True)\n",
        "  evs22 = mtme_data.EvalSet(\"wmt22\", lp, read_stored_metric_scores=True)\n",
        "\n",
        "  mqm_scores_21 = get_metric_scores(evs21, \"mqm\")\n",
        "  mqm_scores_22 = get_metric_scores(evs22, \"mqm\")\n",
        "  metric_scores_21 = get_metric_scores(evs21, metric21)\n",
        "  metric_scores_22 = get_metric_scores(evs22, metric22)\n",
        "\n",
        "  if lp == \"en-de\":\n",
        "    del mqm_scores_21[\"refB\"]\n",
        "    del metric_scores_21[\"refB\"]\n",
        "\n",
        "  X_21, Y_21 = convert_to_matrices(mqm_scores_21, metric_scores_21)\n",
        "  X_22, Y_22 = convert_to_matrices(mqm_scores_22, metric_scores_22)\n",
        "\n",
        "  res21 = tau_optimization.tau_optimization(\n",
        "      X_21.T, Y_21.T, tau_optimization.TauSufficientStats.acc_23\n",
        "  )\n",
        "  res22 = tau_optimization.tau_optimization(\n",
        "      X_22.T, Y_22.T, tau_optimization.TauSufficientStats.acc_23\n",
        "  )\n",
        "\n",
        "  acc21_threshold22 = calculate_group_by_item_acc(X_21, Y_21, res22.best_threshold)\n",
        "  acc22_threshold21 = calculate_group_by_item_acc(X_22, Y_22, res21.best_threshold)\n",
        "\n",
        "  print(metric)\n",
        "  print(f\"WMT'21 eps={res21.best_threshold}, WMT'21 acc={res21.best_tau}, WMT'22 acc={acc22_threshold21}\")\n",
        "  print(f\"WMT'22 eps={res22.best_threshold}, WMT'22 acc={res22.best_tau}, WMT'21 acc={acc21_threshold22}\")\n",
        "  print(f\"WMT'21 accuracy abs delta: {(res21.best_tau - acc21_threshold22) * 100}\")\n",
        "  print(f\"WMT'22 accuracy abs delta: {(res22.best_tau - acc22_threshold21) * 100}\")\n",
        "  print(f\"WMT'21 accuracy rel delta: {(res21.best_tau - acc21_threshold22) / res21.best_tau:.2%}\")\n",
        "  print(f\"WMT'22 accuracy rel delta: {(res22.best_tau - acc22_threshold21) / res22.best_tau:.2%}\")\n",
        "  print(f\"WMT'21 -> 22 epsilon abs delta: {res21.best_threshold - res22.best_threshold}\")\n",
        "  print(f\"WMT'22 -> 21 epsilon abs delta: {res22.best_threshold - res21.best_threshold}\")\n",
        "  print(f\"WMT'21 -> 22 epsilon rel delta: {(res21.best_threshold - res22.best_threshold) / res21.best_threshold:.2%}\")\n",
        "  print(f\"WMT'22 -> 21 epsilon rel delta: {(res22.best_threshold - res21.best_threshold) / res22.best_threshold:.2%}\")\n",
        "\n",
        "\n",
        "  ax.plot(res21.thresholds, res21.taus, label=\"WMT'21\", color=\"blue\")\n",
        "  ax.plot(res22.thresholds, res22.taus, label=\"WMT'22\", color=\"orange\")\n",
        "  ax.axvline(res21.best_threshold, color=\"blue\", linestyle=\"dashed\")\n",
        "  ax.axvline(res22.best_threshold, color=\"orange\", linestyle=\"dashed\")\n",
        "  if ylabel:\n",
        "    ax.set_ylabel(\"Pairwise Accuracy\")\n",
        "  if xlabel:\n",
        "    ax.set_xlabel(\"Epsilon\")\n",
        "  ax.set_title(lp)\n",
        "\n",
        "  elements = [\n",
        "      Patch(facecolor=\"blue\", edgecolor=\"blue\", label=\"WMT'21\"),\n",
        "      Patch(facecolor=\"orange\", edgecolor=\"orange\", label=\"WMT'22\"),\n",
        "      Line2D([0], [0], lw=4, color=\"black\", label=\"Accuracy\"),\n",
        "      Line2D([0], [0], lw=4, color=\"black\", label=\"Best Epsilon\", linestyle=\"dashed\"),\n",
        "  ]\n",
        "  if legend:\n",
        "    ax.legend(handles=elements, frameon=False, handlelength=2.2, handletextpad=0.5, borderpad=0.2)\n",
        "\n",
        "\n",
        "def compare_epsilons_across_years(metric: str):\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 6), sharex=True)\n",
        "  _compare_epsilson_across_years(\"en-de\", metric, axes[0], xlabel=False, ylabel=False)\n",
        "  _compare_epsilson_across_years(\"zh-en\", metric, axes[1], legend=False, ylabel=False)\n",
        "  # Modify space beween subplots\n",
        "  fig.subplots_adjust(hspace=0.2)\n",
        "  # plt.xlim(0, 0.4)\n",
        "\n",
        "  fig.text(0.04, 0.5, 'Pairwise Accuracy', va='center', rotation='vertical')\n",
        "\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEWF8K_eHAGY"
      },
      "outputs": [],
      "source": [
        "compare_epsilons_across_years(\"bleurt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scKYKSSqL6yW"
      },
      "source": [
        "## Ties-F1 Experiment\n",
        "This experiment analyzes what happens if you decompose the accuracy score into F1 scores instead.\n",
        "It corresponds to Figure 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCyouarxJ5iF"
      },
      "outputs": [],
      "source": [
        "def _ties_precision(ss) -> float:\n",
        "  \"\"\"Calculates the precision of metric tie predictions.\"\"\"\n",
        "  denom = ss.ties_both + ss.ties_metric\n",
        "  return ss.ties_both / denom if denom > 0 else 1.0\n",
        "\n",
        "def _ties_recall(ss) -> float:\n",
        "  \"\"\"Calculates the recall of human ties.\"\"\"\n",
        "  denom = ss.ties_both + ss.ties_human\n",
        "  return ss.ties_both / denom if denom > 0 else 1.0\n",
        "\n",
        "def _ties_f1(ss) -> float:\n",
        "  precision = _ties_precision(ss)\n",
        "  recall = _ties_recall(ss)\n",
        "  denom = precision + recall\n",
        "  return 2 * (precision * recall) / denom if denom > 0 else 0.0\n",
        "\n",
        "def _correct_rank_precision(ss) -> float:\n",
        "  denom = ss.con + ss.dis + ss.ties_human\n",
        "  return ss.con / denom if denom > 0 else 1.0\n",
        "\n",
        "def _correct_rank_recall(ss) -> float:\n",
        "  denom = ss.con + ss.dis + ss.ties_metric\n",
        "  return ss.con / denom if denom > 0 else 1.0\n",
        "\n",
        "def _correct_rank_f1(ss) -> float:\n",
        "  \"\"\"Calculates the correct rank F1.\"\"\"\n",
        "  precision = _correct_rank_precision(ss)\n",
        "  recall = _correct_rank_recall(ss)\n",
        "  denom = precision + recall\n",
        "  return 2 * (precision * recall) / denom if denom > 0 else 0.0\n",
        "\n",
        "\n",
        "def plot_other_f1s(lp: str, metric: str):\n",
        "  mqm_scores = get_metric_scores(eval_sets[lp], \"mqm\")\n",
        "  metric_scores = get_metric_scores(eval_sets[lp], metric)\n",
        "\n",
        "  X, Y = convert_to_matrices(mqm_scores, metric_scores)\n",
        "\n",
        "  res = tau_optimization.tau_optimization(\n",
        "      X.T, Y.T, tau_optimization.TauSufficientStats.acc_23\n",
        "  )\n",
        "  thresholds = []\n",
        "  taus_subset = []\n",
        "  ties_p_list, ties_r_list, ties_f1_list = [], [], []\n",
        "  rank_p_list, rank_r_list, rank_f1_list = [], [], []\n",
        "  for i in range(0, len(res.thresholds), 1000):\n",
        "    threshold = res.thresholds[i]\n",
        "    taus_subset.append(res.taus[i])\n",
        "    thresholds.append(threshold)\n",
        "\n",
        "    ties_p, ties_r, ties_f1 = [], [], []\n",
        "    rank_p, rank_r, rank_f1 = [], [], []\n",
        "\n",
        "    for x, y in zip(X.T, Y.T):\n",
        "      con, dis, t_x, t_y, t_xy = mtme_stats._MatrixSufficientStatistics(x, y, threshold, None, None)\n",
        "      ss = tau_optimization.TauSufficientStats(con, dis, t_y, t_x, t_xy)\n",
        "      ties_p.append(_ties_precision(ss))\n",
        "      ties_r.append(_ties_recall(ss))\n",
        "      ties_f1.append(_ties_f1(ss))\n",
        "      rank_p.append(_correct_rank_precision(ss))\n",
        "      rank_r.append(_correct_rank_recall(ss))\n",
        "      rank_f1.append(_correct_rank_f1(ss))\n",
        "\n",
        "    ties_p_list.append(np.mean(ties_p))\n",
        "    ties_r_list.append(np.mean(ties_r))\n",
        "    ties_f1_list.append(np.mean(ties_f1))\n",
        "    rank_p_list.append(np.mean(rank_p))\n",
        "    rank_r_list.append(np.mean(rank_r))\n",
        "    rank_f1_list.append(np.mean(rank_f1))\n",
        "\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 4.5))\n",
        "  axes = [ax]\n",
        "  plt.plot(thresholds, taus_subset, label=\"Accuracy\", color=\"blue\")\n",
        "  plt.plot(thresholds, ties_f1_list, label=\"Ties F1\", color=\"orange\")\n",
        "  plt.plot(thresholds, rank_f1_list, label=\"Correct Rank F1\", color=\"green\")\n",
        "  plt.axvline(res.best_threshold, color=\"blue\", linestyle=\"dashed\", label=\"Best Epsilon\")\n",
        "  plt.xlabel(\"Epsilon\")\n",
        "  plt.xlim(-0.1, 2.0)\n",
        "\n",
        "\n",
        "  handles, labels = axes[0].get_legend_handles_labels()\n",
        "\n",
        "  # Add legend with the modified handles and labels. Disable the legend box,\n",
        "  # which only takes up space and doesn't look good anyway.\n",
        "  # Reshape the line symbol a bit.\n",
        "  axes[0].legend(handles, labels, frameon=False, handlelength=2.2,\n",
        "                 handletextpad=0.5, borderpad=1)\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1uUKM_NNzmG"
      },
      "outputs": [],
      "source": [
        "plot_other_f1s(\"en-de\", \"COMET-22-refA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH8Jaazv-Y_q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "metric",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
